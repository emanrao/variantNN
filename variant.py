"""
Single Nucleotide Polymorphism (SNP) detection for Single Molecule DNA Sequencing

Reassemble sequence from short reads and call variants

Dataset from aggregated alignment BAM file:
    NA12878 PacBio read dataset generated by WUSTL (https://www.ncbi.nlm.nih.gov//bioproject/PRJNA323611)
    Alignment information stored in 3- 15x4 matrices (15 nt, 4 bases coded as one-hot-coding)
    1- baseline encoded refernce sequence and total counts
    2- difference between reference counts and seq counts
    3- difference between reference counts and insertion counts

Align to known variants:
    GIAB project (ftp://ftp-trace.ncbi.nlm.nih.gov:/giab/ftp/release/NA12878_HG001/NISTv3.3.2/GRCh38) 
    List of 15nt sequences containing variant candidates at center postion
    
Train using the high confidence calls on chromosome 21 and test on chromosome 22

Model:
Two convolution + maxpool layers 
Three fully connected layers. 

The output layer contains two group of outputs, each a 4x array. 
 - Base Identity in [A C G T] coding
 - Variant type [het, hom, non-var, other-var] ( Use a softmax layer and cross-entropy for the loss)

Based on VariantNet: https://github.com/pb-jchin/VariantNET
Modified to run on Clusterone
"""

from __future__ import print_function
import sys
import os
import intervaltree
import numpy as np
import random
import tensorflow as tf

#%% Read Training Data
def get_training_array( aln_tensor_fn, variant_set_fn, mask_bed_fn ):
    base2num = dict(zip("ACGT",(0, 1, 2, 3)))
    # Load BED data into interval tree
    tree =  intervaltree.IntervalTree()
    with open(mask_bed_fn) as f:
        for row in f:
            row = row.strip().split()
            b = int(row[1]) # beginning coordinate
            e = int(row[2]) # end coordinate
            tree.addi(b, e, None)
    # Load variants (Labels)
    Y_intitial = {}
    with open( variant_set_fn ) as f:
        for row in f:
            row = row.strip().split()
            if row[3] == "0":  # Het base
                het = True
            else:
                het = False
            
            pos = int(row[0])
            if len(tree.search(pos)) == 0: # not included in BED file
                continue
            base_vec = [0,0,0,0,0,0,0,0]  #[A(0) C(1) G(2) T(3) het(4) hom(5) non-variant(6) non-SNP(7)]
            if het: # split coverage over 2 bases
                base_vec[base2num[row[1][0]]] = 0.5
                base_vec[base2num[row[2][0]]] = 0.5
                base_vec[4] = 1.
            else: # homo base
                base_vec[base2num[row[2][0]]] = 1
                base_vec[5] = 1.

            if len(row[1]) > 1 or len(row[2]) > 1 :  # not simple SNP case
                base_vec[7] = 1.
                base_vec[4] = 0.
                base_vec[5] = 0.
        
            Y_intitial[pos] = base_vec
            
    Y_pos = sorted(Y_intitial.keys())
    cpos = Y_pos[0]
    for pos in Y_pos[1:]: #consider variants w/in 12 bases as non-SNPs
        if abs(pos - cpos) < 12:
            Y_intitial[pos][7] = 1
            Y_intitial[cpos][7] = 1
            
            Y_intitial[pos][4] = 0
            Y_intitial[cpos][4] = 0
            Y_intitial[pos][5] = 0
            Y_intitial[cpos][5] = 0
        cpos = pos

    X_intitial = {}  
    # Load alignment data (Features)
    with open( aln_tensor_fn ) as f:
        for row in f:
            row = row.strip().split()
            pos = int(row[0]) #coordinate
            if len(tree.search(pos)) == 0: # check if included in BED
                continue
            ref_seq = row[1] # reference sequence
            if ref_seq[7] not in ["A","C","G","T"]: # non-standard center base
                continue
            vec = np.reshape(np.array([float(x) for x in row[2:]]), (15,3,4))

            vec = np.transpose(vec, axes=(0,2,1))
            if sum(vec[7,:,0]) < 5:
                continue
            
            vec[:,:,1] -= vec[:,:,0] # subtract matrix 0 from 1 and 2
            vec[:,:,2] -= vec[:,:,0]

            
            X_intitial[pos] = vec
            
            if pos not in Y_intitial: # add corrdinate to Y_initial
                base_vec = [0,0,0,0,0,0,0,0]
                base_vec[base2num[ref_seq[7]]] = 1 # take center base from read
                base_vec[6] = 1. # set as non variant
                Y_intitial[pos] = base_vec
                
    all_pos = sorted(X_intitial.keys())
    random.shuffle(all_pos)

    Xarray = [] # Save array in random order (reads shuffled)
    Yarray = []
    pos_array = []
    for pos in all_pos:
        Xarray.append(X_intitial[pos])
        Yarray.append(Y_intitial[pos])
        pos_array.append(pos)
    Xarray = np.array(Xarray)
    Yarray = np.array(Yarray)

    return Xarray, Yarray, pos_array

def get_batch(X, Y, size=100): # Take random sample of data
    s = random.randint(0,len(X)-size)
    return X[s:s+size], Y[s:s+size]

def get_aln_array( aln_tensor_fn ):

    X_intitial = {}  

    with open( aln_tensor_fn ) as f:
        for row in f:
            row = row.strip().split()
            pos = int(row[0])  # coordinate
            ref_seq = row[1] # reference sequence
            ref_seq = ref_seq.upper()

            if ref_seq[7] not in ["A","C","G","T"]:  #non-standard base at center
                continue

            vec = np.reshape(np.array([float(x) for x in row[2:]]), (15,3,4))

            vec = np.transpose(vec, axes=(0,2,1))
            if sum(vec[7,:,0]) < 5:
                continue
            
            vec[:,:,1] -= vec[:,:,0]
            vec[:,:,2] -= vec[:,:,0]
            
            X_intitial[pos] = vec
                
    all_pos = sorted(X_intitial.keys())

    Xarray = []
    pos_array = []
    for pos in all_pos:
        Xarray.append(X_intitial[pos])
        pos_array.append(pos)
    Xarray = np.array(Xarray)

    return Xarray, pos_array

#%% Variant Caller
class VariantNN(object):

    def __init__(self, input_shape = (15, 4, 3),
                       output_shape1 = (4, ),
                       output_shape2 = (4, ),
                       kernel_size1 = (2, 4),
                       kernel_size2 = (3, 4),
                       poll_size1 = (7, 1),
                       poll_size2 = (3, 1),
                       filter_num = 48,
                       hidden_layer_unit_number = 48):
        self.input_shape = input_shape
        self.output_shape1 = output_shape1
        self.output_shape2 = output_shape2
        self.kernel_size1 = kernel_size1
        self.kernel_size2 = kernel_size2
        self.poll_size1 = poll_size1
        self.poll_size2 = poll_size2
        self.filter_num = filter_num
        self.hidden_layer_unit_number = hidden_layer_unit_number
        self.g = tf.Graph()
        self._build_graph()
        self.session = tf.Session(graph = self.g)


    def _build_graph(self):
        with self.g.as_default():
            X_in = tf.placeholder(tf.float32, [None, self.input_shape[0],
                                                     self.input_shape[1],
                                                     self.input_shape[2]])

            Y_out = tf.placeholder(tf.float32, [None, self.output_shape1[0] + self.output_shape2[0]])

            self.X_in = X_in
            self.Y_out = Y_out
            # Concolution + MaxPool Layer 1
            conv1 = tf.layers.conv2d(
                inputs=X_in,
                filters=self.filter_num,
                kernel_size=self.kernel_size1,
                padding="same",
                activation=tf.nn.elu)
            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=self.poll_size1, strides=1)
            # Concolution + MaxPool Layer 2
            conv2 = tf.layers.conv2d(
                inputs=pool1,
                filters=self.filter_num,
                kernel_size=self.kernel_size2,
                padding="same",
                activation=tf.nn.elu)
            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=self.poll_size2, strides=1)

            flat_size = ( 15 - (self.poll_size1[0] - 1) - (self.poll_size2[0] - 1))
            flat_size *= ( 4 - (self.poll_size1[1] - 1) - (self.poll_size2[1] - 1))
            flat_size *= self.filter_num

            conv2_flat =  tf.reshape(pool2, [-1,  flat_size])
            # Fully Connected + Dropout Layer 1
            unit_num = self.hidden_layer_unit_number
            h1 = tf.layers.dense(inputs=conv2_flat, units=unit_num, activation=tf.nn.elu)
            dropout1 = tf.layers.dropout(inputs=h1, rate=0.50)
            # Fully Connected + Dropout Layer 2
            h2 = tf.layers.dense(inputs=dropout1, units=unit_num, activation=tf.nn.elu)
            dropout2 = tf.layers.dropout(inputs=h2, rate=0.50)
            # Fully Connected + Dropout Layer 3
            h3 = tf.layers.dense(inputs=dropout2, units=unit_num, activation=tf.nn.elu)
            dropout3 = tf.layers.dropout(inputs=h3, rate=0.50)


            Y1 = tf.layers.dense(inputs=dropout3, units=self.output_shape1[0], activation=tf.nn.sigmoid)
            Y2 = tf.layers.dense(inputs=dropout3, units=self.output_shape2[0], activation=tf.nn.elu)
            Y3 = tf.nn.softmax(Y2)

            self.Y1 = Y1
            self.Y3 = Y3

            loss = tf.reduce_sum(  tf.pow( Y1 - tf.slice(Y_out,[0,0],[-1,self.output_shape1[0]] ), 2) )  +\
                   tf.reduce_sum(  tf.nn.softmax_cross_entropy_with_logits( logits=Y2,
                                                                            labels=tf.slice( Y_out, [0,self.output_shape1[0]],
                                                                                                    [-1,self.output_shape2[0]] ) ) )
            self.loss = loss
            learning_rate = 0.0005
            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
            training_op = optimizer.minimize(loss)
            self.training_op = training_op
            self.init_op = tf.global_variables_initializer()

    def init(self):
        self.session.run( self.init_op )

    def close(self):
        self.session.close()

    def train(self, batchX, batchY):
        loss = 0
        X_in = self.X_in
        Y_out = self.Y_out
        loss, _ = self.session.run( (self.loss, self.training_op), feed_dict={X_in:batchX, Y_out:batchY})
        return loss

    def get_loss(self, batchX, batchY):
        loss = 0
        X_in = self.X_in
        Y_out = self.Y_out
        loss  = self.session.run( self.loss, feed_dict={X_in:batchX, Y_out:batchY})
        return loss

    def save_parameters(self, fn):
        with self.g.as_default():
            self.saver = tf.train.Saver()
            self.saver.save(self.session, fn)

    def restore_parameters(self, fn):
        with self.g.as_default():
            self.saver = tf.train.Saver()
            self.saver.restore(self.session, fn)

    def predict(self, Xarray):
        with self.g.as_default():
            bases_, type_  = self.session.run( (self.Y1, self.Y3), feed_dict={self.X_in:Xarray})
            return bases_, type_

    def __del__(self):
        self.session.close()

#%% Main Script - Train Model
try: script_dir = os.path.dirname(__file__)
except:script_dir = os.getcwd()

aln_tensor_fn = os.path.join(script_dir,"data/aln_tensor_chr21")
variant_set_fn = os.path.join(script_dir,"data/variants_chr21")
mask_bed_fn = os.path.join(script_dir,"data/testing_data/chr21/CHROM21_v.3.3.2_highconf_noinconsistent.bed") 
parameter_folder = os.path.join(script_dir,"data/parameters")

# Read in training data
Xarray, Yarray, pos_array = get_training_array(aln_tensor_fn, variant_set_fn, mask_bed_fn)
if not os.path.exists(parameter_folder): os.makedirs(parameter_folder)

# Train Model
vnn = VariantNN()
vnn.init()
batch_size = 500
num_epochs = 2401
validation_lost = []
for i in range(num_epochs):
    Xbatch, Ybatch = get_batch(Xarray[:30000], Yarray[:30000], size=batch_size)
    loss = vnn.train(Xbatch, Ybatch)
    if i % (len(Xarray[:30000])/batch_size) == 0: # validate
        v_lost = vnn.get_loss( Xarray[30000:40000], Yarray[30000:40000] )
        print(i, "train lost:", loss/batch_size, "validation lost", v_lost/10000)
        vnn.save_parameters(parameter_folder +'/vn.params-%04d' % i)
        validation_lost.append( (v_lost, i) )
# pick parameter set with lowest validation loss
validation_lost.sort()
i = validation_lost[0][1]
vnn.restore_parameters(parameter_folder +'/vn.params-%04d' % i)

#%% Main Script - Test Model
aln_tensor_fn = os.path.join(script_dir,"data/aln_tensor_chr22")
variant_set_fn = os.path.join(script_dir,"data/variants_chr22")
mask_bed_fn = os.path.join(script_dir,"data/testing_data/chr22/CHROM22_v.3.3.2_highconf_noinconsistent.bed") 

# Read in test data
Xarray2, Yarray2, pos_array2 = get_training_array(aln_tensor_fn, variant_set_fn, mask_bed_fn) 

base, t = vnn.predict(Xarray2)

evaluation_data = []
for pos, predict_v, annotate_v in zip(np.array(pos_array2), t, Yarray2[:,4:]):
    evaluation_data.append((pos, np.argmax(predict_v), np.argmax(annotate_v)))

ed = np.array(evaluation_data)
recall_het_any = 1.0*sum((ed[:,1]!=2) & (ed[:,2]==0))/sum(ed[:,2]==0)
recall_het_het = 1.0*sum((ed[:,1]==0) & (ed[:,2]==0))/sum(ed[:,2]==0)
ppv_het_any = 1.0*sum((ed[:,1]==0) & (ed[:,2]!=2))/sum(ed[:,1]==0)
ppv_het_het = 1.0*sum((ed[:,1]==0) & (ed[:,2]==0))/sum(ed[:,1]==0)
recall_hom_any = 1.0*sum((ed[:,1]!=2) & (ed[:,2]==1))/sum(ed[:,2]==1)
recall_hom_hom = 1.0*sum((ed[:,1]==1) & (ed[:,2]==1))/sum(ed[:,2]==1)
ppv_hom_any = 1.0*sum((ed[:,1]==1) & (ed[:,2]!=2))/sum(ed[:,1]==1)
ppv_hom_hom = 1.0*sum((ed[:,1]==1) & (ed[:,2]==1))/sum(ed[:,1]==1)
recall_total = 1.0*sum((ed[:,1]!=2) & (ed[:,2]!=2))/sum(ed[:,2]!=2)
ppv_total = 1.0*sum((ed[:,1]!=2) & (ed[:,2]!=2))/sum(ed[:,1]!=2)



print("Recall rate for het-call (regardless called variant types): {:.2f}".format(recall_het_any))
print("Recall rate for het-call (called variant type = het): {:2f}".format(recall_het_any))

print("PPV for het-call (regardless called variant types): {:.2f}".format(recall_het_any))
print("PPV for het-call (called variant type = het): {:2f}".format(recall_het_any))

print("Recall rate for hom-call (regardless called variant types): {:.2f}".format(recall_het_any))
print("Recall rate for hom-call (called variant type = hom): {:.2f}".format(recall_het_any))

print("PPV for hom-call (regardless called variant types): {:.2f}".format(recall_het_any))
print("PPV for hom-call (called variant type = hom): {:.2f}".format(recall_het_any))

print("Recall rate for all calls: {:.2f}".format(recall_het_any))
print("PPV for all calls: {:.2f}".format(recall_het_any))

Xarray3, pos_array3 = get_aln_array(aln_tensor_fn)

all_t = []
for i in range(0, len(Xarray3), 10000):
    base, t = vnn.predict(Xarray3[i:i+10000])
    all_t.append(t)
all_t = np.concatenate(all_t)
evaluation_data2 = []
for pos, predict_v in zip(np.array(pos_array3), all_t):
    evaluation_data2.append((pos, np.argmax(predict_v)))
evaluation_data2 = np.array(evaluation_data2)

print("Total number of variant from the high-confident short-read call-set: ", sum(ed[:,1]!=2))
print("Total number of variant calls from our chr22 data: ", sum(evaluation_data2[:,1] !=
